{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "exceptional-hunger",
      "metadata": {
        "id": "exceptional-hunger"
      },
      "source": [
        "# Machine Learning\n",
        "## Programming Assignment 6: Naive Bayes\n",
        "\n",
        "Instructions:\n",
        "The aim of this assignment is to give you hands-on experience with a real-life machine learning application.\n",
        "You will be analyzing the sentiment of reviews using Naive Bayes classification.\n",
        "You can only use the Python programming language and Jupyter Notebooks.\n",
        "Please use procedural programming style and comment your code thoroughly.\n",
        "There are two parts of this assignment. In part 1, you can use NumPy, Pandas, Matplotlib, and any other standard Python libraries. You are not allowed to use NLTK, scikit-learn, or any other machine learning toolkit. You can only use scikit-learn in part 2."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "alive-brooks",
      "metadata": {
        "id": "alive-brooks"
      },
      "source": [
        "### Part 1: Implementing Naive Bayes classifier from scratch (60 points)\n",
        "\n",
        "You are not allowed to use scikit-learn or any other machine learning toolkit for this part. You have to implement your own Naive Bayes classifier from scratch. You may use Pandas, NumPy, Matplotlib, and other standard Python libraries.\n",
        "\n",
        "#### Problem:\n",
        "The purpose of this assignment is to get you familiar with Naive Bayes classification. The core dataset contains 50,000 reviews split evenly into 25k train and 25k test sets. The overall distribution of labels is balanced (25k pos and 25k neg). There are two top-level directories [train/, test/] corresponding to the training and test sets. Each contains [pos/, neg/] directories for the reviews with binary labels positive and negative. Within these directories, reviews are stored in text files named following the convention [[id]_[rating].txt] where [id] is a unique id and [rating] is the star rating for that review on a 1-10 scale. For example, the file [test/pos/200_8.txt] is the text for a positive-labeled testset example with unique id 200 and star rating 8/10 from IMDb.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "solid-placement",
      "metadata": {
        "id": "solid-placement"
      },
      "outputs": [],
      "source": [
        "## Here are the libraries you will need for this part/\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.spatial as sc\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import random\n",
        "%matplotlib inline\n",
        "## Here we have added the standard libraries\n",
        "import zipfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "educated-bandwidth",
      "metadata": {
        "id": "educated-bandwidth"
      },
      "source": [
        "#### Task 1.1: Dataset (5 points)\n",
        "Your task is to read the dataset and stopwords file into a useful data structure. Print out a few reviews and a few items from the stop word list, succesfully being able to do this will earn you 5 points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "indoor-prediction",
      "metadata": {
        "id": "indoor-prediction",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a84a91-7459-4f64-f244-03ed8f4c3620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop Words: ['i', \"i'm\", 'me', 'my', 'myself']\n",
            "1. Label: 1 | Review: I never thought an old cartoon would bring tears to my eyes! When I first purchased Casper & Friends: Spooking About Africa, I so much wanted to see the very first Casper cartoon entitled The Friendly Ghost (1945), But when I saw the next cartoon, There's Good Boos To-Night (1948), It made me break down! I couldn't believe how sad and tragic it was after seeing Casper's fox get killed! I never saw anything like that in the other Casper cartoons! This is the saddest one of all! It was so depressing, I just couldn't watch it again. It's just like seeing Lassie die at the end of a movie. I know it's a classic,But it's too much for us old cartoon fans to handle like me! If I wanted to watch something old and classic, I rather watch something happy and funny! But when I think about this Casper cartoon, I think about my cats!\n",
            "2. Label: 0 | Review: If the writer/director is reading this (and I imagine you are since you should now be out of work) then I must tell you - I have seen some bad movies in my time but this one gets the distinction of having the worst premise I've ever heard.<br /><br />SPOILERS - Nothing happens! <br /><br />A total waste of time. I laughed out loud at the end. <br /><br />SIDE NOTE - (if the whole movie was her in a coma then does the scene where she sleeps with that guy mean someone raped her while she was knocked out?)<br /><br />Utter rubbish.\n",
            "3. Label: 1 | Review: I know this sounds odd coming from someone born almost 15 years after the show stopped airing, but I love this show. I don't know why, but I enjoy watching it. I love Adam the best. The only disappointing thing is that the only place I found to buy the seasons on DVD was in Germany, and that was only the first two seasons. That is disappointing, but that's OK. I'll keep looking online. If anyone has any tips on where to buy the second through 14th seasons, please email me at darkangel_1627@yahoo.com. I already own the first one. The only down side is that the DVDs being from Germany, they only play on my portable DVD player and my computer. Oh well. I still own it!\n",
            "4. Label: 0 | Review: Kathryn Bigelow and Mark Boal are already preparing a sequel about a young US corporal in Afghanistan. He also happens to be a highly-qualified surgeon and is roaming freely around Kabul, operating on wounded NATO soldiers. On a particularly difficult mission, he casually picks up a sniper rifle and shoots Osama Bin Laden from a distance of about 3000 yards. He is then finally promoted to sergeant, but is unable to decide between a sniper and surgeon career, so he quits from the Army altogether. One year later, frustrated with civilian life, he joins the Navy and the last scene shows him proudly wearing a white uniform.\n",
            "5. Label: 1 | Review: Alain Chabat claims this movie as his original idea but the theme of reluctant lovers who finally get it together is as old, if not older, than Shakespeare.<br /><br />Chabat is a \"vieux garcon\", happily single and not wanting any member of the opposite sex to disturb his life. He has a problem, 5 sisters and a matriarchal mum - the G7 - who decide he should be married. Enter the delightful, charming Charlotte Gainsbourg and what should be a simple plan. Charlotte has to pose as Chabat's girlfriend and then simply not turn up on the day of the wedding. No more talk of marriage from the G7. Of course the best laid plans have a habit of spiralling out of control.<br /><br />There are very strong supporting roles from Lafont as the mother and Osterman as the tight-fisted brother of Gainsbourg.<br /><br />There are some fantastic scenes as first Charlotte has to charm, then revolt the family. French farce with an English.\n"
          ]
        }
      ],
      "source": [
        "zip_path = 'Naive Bayes Data.zip'\n",
        "extract_to = './naive_bayes_data'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "stopwords_path = os.path.join(extract_to, 'stop_words.txt') ## to load stop words from the stop_words.txt file\n",
        "with open(stopwords_path, 'r') as f:\n",
        "    stop_words = f.read().split()\n",
        "print(\"Stop Words:\", stop_words[:5])\n",
        "def load_reviews_from_folder(folder_path, label): ## to load all reviews and assign a label to them\n",
        "    reviews = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "                reviews.append((content, label))\n",
        "    return reviews\n",
        "train_pos = load_reviews_from_folder(os.path.join(extract_to, 'train', 'pos'), label=1) ## to load positive and negative training data\n",
        "train_neg = load_reviews_from_folder(os.path.join(extract_to, 'train', 'neg'), label=0)\n",
        "test_pos = load_reviews_from_folder(os.path.join(extract_to, 'test', 'pos'), label=1) ## to load positive and negative testing data\n",
        "test_neg = load_reviews_from_folder(os.path.join(extract_to, 'test', 'neg'), label=0)\n",
        "train_data = train_pos + train_neg ## ombine positive and negative reviews\n",
        "test_data = test_pos + test_neg\n",
        "random.shuffle(train_data)\n",
        "random.shuffle(test_data)\n",
        "##OUTPUT (the rubric requires minimum two however I have printed five tweets and five words)\n",
        "for i in range(5):\n",
        "    print(f\"{i+1}. Label: {train_data[i][1]} | Review: {train_data[i][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ignored-reminder",
      "metadata": {
        "id": "ignored-reminder"
      },
      "source": [
        "#### Task 1.2: Data Preprocessing (10 points)\n",
        "\n",
        "In the preprocessing step, youâ€™re required to remove the stop words, punctuation marks, numbers, unwanted symbols, hyperlinks, and usernames from the tweets and convert them to lower case. You may find the string and regex module useful for this purpose. Use the stop word list provided within the assignment.\n",
        "\n",
        "Print out a few random reviews from your dataset, if they conform to the rules mentioned above, you will gain 10 points."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, stop_words):\n",
        "    text = text.lower() ## to convert to lowercase\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text) ## to remove hyperlinks\n",
        "    text = re.sub(r'@\\w+', '', text) ## to remove usernames\n",
        "    text = re.sub(r'[^a-z\\s]', '', text) # to remove all non-alphabetic characters\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words] ## to remove stop words\n",
        "    return ' '.join(words)\n",
        "processed_reviews = [(preprocess_text(review, stop_words), label) for review, label in train_data]\n",
        "for i in range(5):\n",
        "    print(f\"{i+1}.\", processed_reviews[i][0])\n",
        "print(\"\\nWords from the tweets shown in quotes as per PA6 tutorial:\")\n",
        "for i in range(5):\n",
        "    words = processed_reviews[i][0].split()\n",
        "    quoted_words = ', '.join([f\"'{word}'\" for word in words])\n",
        "    print(f\"{i+1}. {quoted_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LwEJY_xXorT",
        "outputId": "647ee6c5-ffd8-4f20-aa7f-2d342f755507"
      },
      "id": "0LwEJY_xXorT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. never thought old cartoon would bring tears eyes first purchased casper friends spooking africa much wanted see first casper cartoon entitled friendly ghost saw next cartoon theres good boos tonight made break couldnt believe sad tragic seeing caspers fox get killed never saw anything like casper cartoons saddest one depressing couldnt watch like seeing lassie die end movie know classicbut much us old cartoon fans handle like wanted watch something old classic rather watch something happy funny think casper cartoon think cats\n",
            "2. writerdirector reading imagine since work must tell seen bad movies time one gets distinction worst premise ive ever heardbr br spoilers nothing happens br br total waste time laughed loud end br br side note whole movie coma scene sleeps guy mean someone raped knocked outbr br utter rubbish\n",
            "3. know sounds odd coming someone born almost years show stopped airing love show dont know enjoy watching love adam best disappointing thing place found buy seasons dvd germany first two seasons disappointing thats ok ill keep looking online anyone tips buy second th seasons please email darkangelcom already first one side dvds germany play portable dvd player computer oh well still\n",
            "4. kathryn bigelow mark boal already preparing sequel young us corporal afghanistan also happens highlyqualified surgeon roaming freely around kabul operating wounded nato soldiers particularly difficult mission casually picks sniper rifle shoots osama bin laden distance yards finally promoted sergeant unable decide sniper surgeon career quits army altogether one year later frustrated civilian life joins navy last scene shows proudly wearing white uniform\n",
            "5. alain chabat claims movie original idea theme reluctant lovers finally get together old older shakespearebr br chabat vieux garcon happily single wanting member opposite sex disturb life problem sisters matriarchal mum g decide married enter delightful charming charlotte gainsbourg simple plan charlotte pose chabats girlfriend simply turn day wedding talk marriage g course best laid plans habit spiralling controlbr br strong supporting roles lafont mother osterman tightfisted brother gainsbourgbr br fantastic scenes first charlotte charm revolt family french farce english\n",
            "\n",
            "Words from the tweets shown in quotes as per PA6 tutorial:\n",
            "1. 'never', 'thought', 'old', 'cartoon', 'would', 'bring', 'tears', 'eyes', 'first', 'purchased', 'casper', 'friends', 'spooking', 'africa', 'much', 'wanted', 'see', 'first', 'casper', 'cartoon', 'entitled', 'friendly', 'ghost', 'saw', 'next', 'cartoon', 'theres', 'good', 'boos', 'tonight', 'made', 'break', 'couldnt', 'believe', 'sad', 'tragic', 'seeing', 'caspers', 'fox', 'get', 'killed', 'never', 'saw', 'anything', 'like', 'casper', 'cartoons', 'saddest', 'one', 'depressing', 'couldnt', 'watch', 'like', 'seeing', 'lassie', 'die', 'end', 'movie', 'know', 'classicbut', 'much', 'us', 'old', 'cartoon', 'fans', 'handle', 'like', 'wanted', 'watch', 'something', 'old', 'classic', 'rather', 'watch', 'something', 'happy', 'funny', 'think', 'casper', 'cartoon', 'think', 'cats'\n",
            "2. 'writerdirector', 'reading', 'imagine', 'since', 'work', 'must', 'tell', 'seen', 'bad', 'movies', 'time', 'one', 'gets', 'distinction', 'worst', 'premise', 'ive', 'ever', 'heardbr', 'br', 'spoilers', 'nothing', 'happens', 'br', 'br', 'total', 'waste', 'time', 'laughed', 'loud', 'end', 'br', 'br', 'side', 'note', 'whole', 'movie', 'coma', 'scene', 'sleeps', 'guy', 'mean', 'someone', 'raped', 'knocked', 'outbr', 'br', 'utter', 'rubbish'\n",
            "3. 'know', 'sounds', 'odd', 'coming', 'someone', 'born', 'almost', 'years', 'show', 'stopped', 'airing', 'love', 'show', 'dont', 'know', 'enjoy', 'watching', 'love', 'adam', 'best', 'disappointing', 'thing', 'place', 'found', 'buy', 'seasons', 'dvd', 'germany', 'first', 'two', 'seasons', 'disappointing', 'thats', 'ok', 'ill', 'keep', 'looking', 'online', 'anyone', 'tips', 'buy', 'second', 'th', 'seasons', 'please', 'email', 'darkangelcom', 'already', 'first', 'one', 'side', 'dvds', 'germany', 'play', 'portable', 'dvd', 'player', 'computer', 'oh', 'well', 'still'\n",
            "4. 'kathryn', 'bigelow', 'mark', 'boal', 'already', 'preparing', 'sequel', 'young', 'us', 'corporal', 'afghanistan', 'also', 'happens', 'highlyqualified', 'surgeon', 'roaming', 'freely', 'around', 'kabul', 'operating', 'wounded', 'nato', 'soldiers', 'particularly', 'difficult', 'mission', 'casually', 'picks', 'sniper', 'rifle', 'shoots', 'osama', 'bin', 'laden', 'distance', 'yards', 'finally', 'promoted', 'sergeant', 'unable', 'decide', 'sniper', 'surgeon', 'career', 'quits', 'army', 'altogether', 'one', 'year', 'later', 'frustrated', 'civilian', 'life', 'joins', 'navy', 'last', 'scene', 'shows', 'proudly', 'wearing', 'white', 'uniform'\n",
            "5. 'alain', 'chabat', 'claims', 'movie', 'original', 'idea', 'theme', 'reluctant', 'lovers', 'finally', 'get', 'together', 'old', 'older', 'shakespearebr', 'br', 'chabat', 'vieux', 'garcon', 'happily', 'single', 'wanting', 'member', 'opposite', 'sex', 'disturb', 'life', 'problem', 'sisters', 'matriarchal', 'mum', 'g', 'decide', 'married', 'enter', 'delightful', 'charming', 'charlotte', 'gainsbourg', 'simple', 'plan', 'charlotte', 'pose', 'chabats', 'girlfriend', 'simply', 'turn', 'day', 'wedding', 'talk', 'marriage', 'g', 'course', 'best', 'laid', 'plans', 'habit', 'spiralling', 'controlbr', 'br', 'strong', 'supporting', 'roles', 'lafont', 'mother', 'osterman', 'tightfisted', 'brother', 'gainsbourgbr', 'br', 'fantastic', 'scenes', 'first', 'charlotte', 'charm', 'revolt', 'family', 'french', 'farce', 'english'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "impossible-potato",
      "metadata": {
        "id": "impossible-potato"
      },
      "source": [
        "#### Task 1.3: Splitting the dataset (5 points)\n",
        "\n",
        "In this part, divide the given dataset into training and testing sets based on an 80-20 split using python.\n",
        "Print out the sizes of the training dataset and test dataset, training data should contain 40000 reviews and test data should contain 10000 reviews. If your sizes are correct, you get full points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "relevant-episode",
      "metadata": {
        "id": "relevant-episode",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f9e751-eefe-4b17-d094-1820ba82b1ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 40000 reviews\n",
            "Test set size: 10000 reviews\n",
            "Total dataset size (after combining train and test folders): 50000\n"
          ]
        }
      ],
      "source": [
        "all_data = train_data + test_data ## combine both training and testing data\n",
        "processed_reviews = [(preprocess_text(review, stop_words), label) for review, label in all_data]\n",
        "split_index = int(0.8 * len(processed_reviews)) ## split the data 80-20 split\n",
        "train_split = processed_reviews[:split_index]\n",
        "test_split = processed_reviews[split_index:]\n",
        "## OUTPUT to display the sizes\n",
        "print(\"Training set size:\", len(train_split), \"reviews\")\n",
        "print(\"Test set size:\", len(test_split), \"reviews\")\n",
        "print(\"Total dataset size (after combining train and test folders):\", len(processed_reviews))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "julian-venice",
      "metadata": {
        "id": "julian-venice"
      },
      "source": [
        "#### Task 1.4: Create Naive Bayes classifier (30 points)\n",
        "\n",
        "You will create your own Naive Neighbors classifier function by implementing the following algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "southwest-consumption",
      "metadata": {
        "id": "southwest-consumption"
      },
      "outputs": [],
      "source": [
        "##from IPython.display import Image, display\n",
        "##display(Image(filename='NBAlgo.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "consolidated-fortune",
      "metadata": {
        "id": "consolidated-fortune",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1842ab-9927-463d-c875-80b0a05aeaa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary created with 155005 unique words.\n",
            "Classifier test run: predicted 0, actual 0\n",
            "The vocabulary has been made and the classifier is running perfectly by returning the argmax of the likelihood.\n",
            "\n",
            "Sample Predictions:\n",
            "Tweet (actual: negative):\n",
            "'makes movie damn bad lame subpar juvenile humor could horrid trendy suck ass music perhaps uninspired go nowhere story maybe even fact traci lords gives worst acting performance ever add insult injury keeps clothes throughout length steaming turd sandwich regardless matter reason film sucks fact remains really really never wished could watching movie dean cameron instead watching life ski school masterpiece comic genius compared travestybr br grade f br br eye candy nikol nesbitt buffy tyler suzanne stokes unleash tupperware titsbr br saw starz demand'\n",
            "Predicted: negative\n",
            "Tweet (actual: positive):\n",
            "'recently saw movie first time enjoyed much went right bought dvd movie pure genius gets funnier viewing anyone write jokes funny dialog actors memorize basis movie improve thank christopher guest'\n",
            "Predicted: negative\n",
            "\n",
            "Logprior:\n",
            "  Class 1: -0.6928472255509474\n",
            "  Class 0: -0.6934472255689473\n",
            "\n",
            "Sample Loglikelihoods:\n",
            "  Word 'detailor' | Class 1: -14.08641059653082\n",
            "  Word 'blinkandyoullmisshim' | Class 1: -14.779557777090766\n",
            "  Word 'marvinrob' | Class 1: -14.779557777090766\n",
            "  Word 'fantastico' | Class 1: -14.08641059653082\n",
            "  Word 'norikos' | Class 1: -13.170119864656664\n",
            "\n",
            "Accuracy on test set: 85.01 %\n"
          ]
        }
      ],
      "source": [
        "def train_naive_bayes(data): ## to train a Naive Bayes classifier\n",
        "    bigdoc = {}\n",
        "    label_counts = {}\n",
        "    vocab = set()\n",
        "    for text, label in data: ## to loop through each review and aggregate words by class\n",
        "        words = text.split()\n",
        "        if label not in label_counts:\n",
        "            label_counts[label] = 0\n",
        "            bigdoc[label] = []\n",
        "        label_counts[label] += 1\n",
        "        bigdoc[label].extend(words)\n",
        "        vocab.update(words)\n",
        "    total_docs = len(data)\n",
        "    V = vocab ## vocabulary set\n",
        "    logprior = {} ## to store log prior\n",
        "    loglikelihood = {} ## to store log likelihood\n",
        "    for c in label_counts:\n",
        "        logprior[c] = np.log(label_counts[c] / total_docs) ## to compute logprior and loglikelihood\n",
        "    for c in label_counts:\n",
        "        word_counts = {}\n",
        "        for word in bigdoc[c]:\n",
        "            word_counts[word] = word_counts.get(word, 0) + 1\n",
        "        total_wc = sum(word_counts.values())\n",
        "        for word in V:\n",
        "            count = word_counts.get(word, 0)\n",
        "            loglikelihood[(word, c)] = np.log((count + 1) / (total_wc + len(V)))\n",
        "    return logprior, loglikelihood, V\n",
        "def predict(text, logprior, loglikelihood, V): ## to predict the class of a given text based on trained model\n",
        "    words = text.split()\n",
        "    scores = {}\n",
        "    for c in logprior:\n",
        "        scores[c] = logprior[c]\n",
        "        for word in words:\n",
        "            if word in V:\n",
        "                scores[c] += loglikelihood.get((word, c), 0)\n",
        "    return max(scores, key=scores.get) ## to return the class with the highest score\n",
        "train_data = train_split ## already preprocessed in Task 1.3\n",
        "test_data = test_split\n",
        "logprior, loglikelihood, V = train_naive_bayes(train_data)\n",
        "print(f\"Vocabulary created with {len(V)} unique words.\") ## to display vocabulary size\n",
        "sample_text = test_data[0][0] ## to confirm that the classifier is running and returning predictions correctly\n",
        "sample_label = test_data[0][1]\n",
        "sample_pred = predict(sample_text, logprior, loglikelihood, V)\n",
        "print(f\"Classifier test run: predicted {sample_pred}, actual {sample_label}\")\n",
        "print(\"The vocabulary has been made and the classifier is running perfectly by returning the argmax of the likelihood.\\n\")\n",
        "print(\"Sample Predictions:\") ## to display sample predictions\n",
        "shown_pos = shown_neg = 0\n",
        "for text, label in test_data:\n",
        "    pred = predict(text, logprior, loglikelihood, V)\n",
        "    if label == 1 and shown_pos == 0:\n",
        "        print(\"Tweet (actual: positive):\")\n",
        "        print(f\"'{text}'\")\n",
        "        print(f\"Predicted: {'positive' if pred == 1 else 'negative'}\")\n",
        "        shown_pos = 1\n",
        "    if label == 0 and shown_neg == 0:\n",
        "        print(\"Tweet (actual: negative):\")\n",
        "        print(f\"'{text}'\")\n",
        "        print(f\"Predicted: {'positive' if pred == 1 else 'negative'}\")\n",
        "        shown_neg = 1\n",
        "    if shown_pos and shown_neg:\n",
        "        break\n",
        "print(\"\\nLogprior:\") # to display the prior log probabilities for each class\n",
        "for c in logprior:\n",
        "    print(f\"  Class {c}: {logprior[c]}\")\n",
        "print(\"\\nSample Loglikelihoods:\") ## to display a few loglikelihood values\n",
        "i = 0\n",
        "for key in loglikelihood:\n",
        "    print(f\"  Word '{key[0]}' | Class {key[1]}: {loglikelihood[key]}\")\n",
        "    i += 1\n",
        "    if i == 5:\n",
        "        break\n",
        "correct = 0 ## to calculate accuracy\n",
        "for text, label in test_data:\n",
        "    pred = predict(text, logprior, loglikelihood, V)\n",
        "    if pred == label:\n",
        "        correct += 1\n",
        "accuracy = correct / len(test_data) ## to calculate accuracy\n",
        "print(\"\\nAccuracy on test set:\", round(accuracy * 100, 2), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "durable-equivalent",
      "metadata": {
        "id": "durable-equivalent"
      },
      "source": [
        "#### Task 1.5: Implement evaluation functions (10 points)\n",
        "\n",
        "Implement evaluation functions that calculates the:\n",
        "- classification accuracy,\n",
        "- F1 score,\n",
        "- and the confusion matrix\n",
        "of your classifier on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(predictions, true_labels): ## to evaluate prediction results\n",
        "    tp = sum(1 for p, t in zip(predictions, true_labels) if p == 1 and t == 1) ## count true positives\n",
        "    tn = sum(1 for p, t in zip(predictions, true_labels) if p == 0 and t == 0) ## count true negatives\n",
        "    fp = sum(1 for p, t in zip(predictions, true_labels) if p == 1 and t == 0) ## count false positives\n",
        "    fn = sum(1 for p, t in zip(predictions, true_labels) if p == 0 and t == 1) ## count false negatives\n",
        "    accuracy = (tp + tn) / len(predictions) ## to calculate accuracy\n",
        "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
        "    recall    = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "    f1        = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "    return accuracy * 100, f1 * 100, [[tp, fn], [fp, tn]]\n",
        "preds = [predict(text, logprior, loglikelihood, V) for text, _ in test_data] ## to predict on test data\n",
        "true = [label for _, label in test_data]\n",
        "acc, f1, cm = evaluate(preds, true)\n",
        "##OUTPUT to print accuracy, F1 score, and confusion matrix\n",
        "print(\"Accuracy:\", round(acc, 2), \"%\")\n",
        "print(\"F1 Score:\", round(f1, 2), \"%\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"{:<12} {:<15} {:<15}\".format(\"\", \"Predicted Pos\", \"Predicted Neg\"))\n",
        "print(\"{:<12} {:<15} {:<15}\".format(\"Actual Pos\", cm[0][0], cm[0][1]))\n",
        "print(\"{:<12} {:<15} {:<15}\".format(\"Actual Neg\", cm[1][0], cm[1][1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXqekIGghjSP",
        "outputId": "c5eced62-7491-4c5c-e944-ba52508be83d"
      },
      "id": "JXqekIGghjSP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 85.01 %\n",
            "F1 Score: 84.48 %\n",
            "Confusion Matrix:\n",
            "             Predicted Pos   Predicted Neg  \n",
            "Actual Pos   4079            915            \n",
            "Actual Neg   584             4422           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "angry-diversity",
      "metadata": {
        "id": "angry-diversity"
      },
      "source": [
        "### Part 2:  Naive Bayes classifier using scikit-learn (40 points)\n",
        "\n",
        "In this part, use scikit-learnâ€™s CountVectorizer to transform your train and test set to bag-of-words representation and NaÃ¯ve Bayes implementation to train and test the NaÃ¯ve Bayes on the provided dataset. Use scikit-learnâ€™s accuracy_score function to calculate the accuracy and confusion_matrix function to calculate the confusion matrix on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "intelligent-fundamental",
      "metadata": {
        "id": "intelligent-fundamental"
      },
      "outputs": [],
      "source": [
        "# Here are the libraries and specific functions you will be needing for this part\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tough-crazy",
      "metadata": {
        "id": "tough-crazy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0177f085-c4e4-4115-f144-dc707a6bf994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 85.03 %\n",
            "Confusion Matrix:\n",
            "             Predicted Pos   Predicted Neg  \n",
            "Actual Pos   4079            915            \n",
            "Actual Neg   582             4424           \n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86      5006\n",
            "           1       0.88      0.82      0.84      4994\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "all_data = train_data + test_data\n",
        "processed_reviews = [(preprocess_text(review, stop_words), label) for review, label in all_data]\n",
        "split_index = int(0.8 * len(processed_reviews))\n",
        "train_split = processed_reviews[:split_index]\n",
        "test_split = processed_reviews[split_index:]\n",
        "train_texts = [text for text, label in train_split] # to separate the text data and corresponding labels for training and testing\n",
        "train_labels = [label for text, label in train_split]\n",
        "test_texts = [text for text, label in test_split]\n",
        "test_labels = [label for text, label in test_split]\n",
        "vectorizer = CountVectorizer(stop_words=stop_words) ## to transform the text data into a bag-of-words representation\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "X_test = vectorizer.transform(test_texts)\n",
        "clf = MultinomialNB() ## to train a Naive Bayes classifier\n",
        "clf.fit(X_train, train_labels)\n",
        "predictions = clf.predict(X_test)\n",
        "acc = accuracy_score(test_labels, predictions)\n",
        "cm = confusion_matrix(test_labels, predictions)\n",
        "report = classification_report(test_labels, predictions)\n",
        "##OUTPUT to print accuracy, confusion matrix and classification report\n",
        "print(\"Accuracy:\", round(acc * 100, 2), \"%\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"{:<12} {:<15} {:<15}\".format(\"\", \"Predicted Pos\", \"Predicted Neg\"))\n",
        "print(\"{:<12} {:<15} {:<15}\".format(\"Actual Pos\", cm[1][1], cm[1][0]))\n",
        "print(\"{:<12} {:<15} {:<15}\".format(\"Actual Neg\", cm[0][1], cm[0][0]))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}