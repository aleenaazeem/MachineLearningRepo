{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "exceptional-hunger",
      "metadata": {
        "id": "exceptional-hunger"
      },
      "source": [
        "# Machine Learning\n",
        "## Programming Assignment 6: Naive Bayes\n",
        "\n",
        "Instructions:\n",
        "The aim of this assignment is to give you hands-on experience with a real-life machine learning application.\n",
        "You will be analyzing the sentiment of reviews using Naive Bayes classification.\n",
        "You can only use the Python programming language and Jupyter Notebooks.\n",
        "Please use procedural programming style and comment your code thoroughly.\n",
        "There are two parts of this assignment. In part 1, you can use NumPy, Pandas, Matplotlib, and any other standard Python libraries. You are not allowed to use NLTK, scikit-learn, or any other machine learning toolkit. You can only use scikit-learn in part 2."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "alive-brooks",
      "metadata": {
        "id": "alive-brooks"
      },
      "source": [
        "### Part 1: Implementing Naive Bayes classifier from scratch (60 points)\n",
        "\n",
        "You are not allowed to use scikit-learn or any other machine learning toolkit for this part. You have to implement your own Naive Bayes classifier from scratch. You may use Pandas, NumPy, Matplotlib, and other standard Python libraries.\n",
        "\n",
        "#### Problem:\n",
        "The purpose of this assignment is to get you familiar with Naive Bayes classification. The core dataset contains 50,000 reviews split evenly into 25k train and 25k test sets. The overall distribution of labels is balanced (25k pos and 25k neg). There are two top-level directories [train/, test/] corresponding to the training and test sets. Each contains [pos/, neg/] directories for the reviews with binary labels positive and negative. Within these directories, reviews are stored in text files named following the convention [[id]_[rating].txt] where [id] is a unique id and [rating] is the star rating for that review on a 1-10 scale. For example, the file [test/pos/200_8.txt] is the text for a positive-labeled testset example with unique id 200 and star rating 8/10 from IMDb.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "solid-placement",
      "metadata": {
        "id": "solid-placement"
      },
      "outputs": [],
      "source": [
        "## Here are the libraries you will need for this part/\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.spatial as sc\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import random\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "educated-bandwidth",
      "metadata": {
        "id": "educated-bandwidth"
      },
      "source": [
        "#### Task 1.1: Dataset (5 points)\n",
        "Your task is to read the dataset and stopwords file into a useful data structure. Print out a few reviews and a few items from the stop word list, succesfully being able to do this will earn you 5 points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "indoor-prediction",
      "metadata": {
        "id": "indoor-prediction",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f377df10-ef5e-4c21-ed80-babe685f48d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of dataset directory:\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Unzip the uploaded file\n",
        "with zipfile.ZipFile(\"/content/Naive Bayes Data.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Define a function to read random reviews from a directory\n",
        "def read_sample_reviews(directory, count=3):\n",
        "    files = os.listdir(directory)\n",
        "    chosen_files = random.sample(files, count)\n",
        "    for fname in chosen_files:\n",
        "        with open(os.path.join(directory, fname), 'r', encoding='utf-8') as f:\n",
        "            print(f\"\\n--- {fname} ---\\n{f.read()[:500]}\")  # show first 500 chars\n",
        "\n",
        "# Read some stopwords\n",
        "stopwords = []\n",
        "with open(\"/content/stop_words.txt\", 'r') as f:\n",
        "    stopwords = f.read().splitlines()\n",
        "\n",
        "print(\"\\nSample stopwords:\", stopwords[:10])  # Print first 10 stopwords\n",
        "\n",
        "# Read 3 random reviews from each category\n",
        "print(\"\\nüìÇ train/pos\")\n",
        "read_sample_reviews(\"/content/train/pos\")\n",
        "\n",
        "print(\"\\nüìÇ train/neg\")\n",
        "read_sample_reviews(\"/content/train/neg\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzIhw_fhmBiu",
        "outputId": "581e4f12-438b-4d0d-db21-3f8e50c468b6"
      },
      "id": "lzIhw_fhmBiu",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample stopwords: ['i', \"i'm\", 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you']\n",
            "\n",
            "üìÇ train/pos\n",
            "\n",
            "--- 10859_7.txt ---\n",
            "Not to be confused with the Resse Witherspoon high school film of the same name, this is a stylised look at Hong Kong's triad gangs. Called election because a new leader or 'chairman' is elected by ancient traditions every two years. Two candidates are up for the position and through ego, bribes and past track record the race is tense to say the least. Expertly directed to introduce you to an expansive cast without ever being confusing the story twists and turns before revealing itself in all it\n",
            "\n",
            "--- 10181_8.txt ---\n",
            "I initially bought this DVD because it had SRK and Aishwarya Rai on the cover and I thought, hey! another film starring Aishu and Shah Rukh, little did I know that Aishwarya would only appear in an item number in the last quarter of the film in a song which she shares with SRK and helps introduce his character who is in the film for about just 15 minutes. Shakti is a film about a mother's love and endurance. It's a film about transformations, ignorance, coming of age, stepping into the know and \n",
            "\n",
            "--- 9923_7.txt ---\n",
            "I read several mixed reviews and several of them downright trashed the movie. I originally became interested in this project because it was being directed by Tony Scott and I have become very interested in his work after Man On Fire had such a profound impact on me. Before I start my review, let me first say this...it's wonderful to see that this movie could have been told in a boring and ordinary manner, yet the writers and Scott chose a different approach.<br /><br />Plot:<br /><br />Simply st\n",
            "\n",
            "üìÇ train/neg\n",
            "\n",
            "--- 2704_1.txt ---\n",
            "I don't understand why people would praise this garbage. Its wrong , stupid , unrealistic , awful , and just about everything else. The film is a view on life , racial issues , prejudice , and everything else that strangely goes on in College. This is where it fails. It has no grasps on reality. From many questionable non-sense scenes in the movie such as for example<br /><br />A black man chasing down a white man with a gun, the black man and stopped by the security guards handcuffed and carrie\n",
            "\n",
            "--- 6259_3.txt ---\n",
            "Well, maybe the PC version of this game was impressive. Maybe. I just finished playing the PS2 version and it's pretty much a complete mess.<br /><br />There are a couple elements that are okay or promising. I'll mention those first because it will be over quickly. First, the idea of a historical GTA-like game is a great one. The game Gun was a historical GTA-like game and unlike Mafia, Gun was excellent. I'd love to see a game set during Mafia's era done right. Next, the storyline is well writt\n",
            "\n",
            "--- 9606_4.txt ---\n",
            "\"La Lupa Mannara\" aka. \"Werewolf Woman\" of 1976 is a film with a highly promising title, but, sadly, the film itself is pretty far away from being a must-see for my fellow Italian Horror buffs. You won't hear me say that Rino Di Silvestri's film is entirely bad - it has its stylish moments, and the first half is actually great fun to watch (though the fun is unintentional). The film also profits from an exceptionally exhibitionist leading actress, Annik Borel. However, the film, which has no rea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ignored-reminder",
      "metadata": {
        "id": "ignored-reminder"
      },
      "source": [
        "#### Task 1.2: Data Preprocessing (10 points)\n",
        "\n",
        "In the preprocessing step, you‚Äôre required to remove the stop words, punctuation marks, numbers, unwanted symbols, hyperlinks, and usernames from the tweets and convert them to lower case. You may find the string and regex module useful for this purpose. Use the stop word list provided within the assignment.\n",
        "\n",
        "Print out a few random reviews from your dataset, if they conform to the rules mentioned above, you will gain 10 points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "hydraulic-patch",
      "metadata": {
        "id": "hydraulic-patch"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def preprocess_text(text, stopwords):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove user mentions and hashtags\n",
        "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove numbers and special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    tokens = text.split()\n",
        "    cleaned_tokens = [word for word in tokens if word not in stopwords]\n",
        "\n",
        "    # Reconstruct cleaned review\n",
        "    return ' '.join(cleaned_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick 3 random reviews from train/pos, clean and print\n",
        "sample_dir = \"/content/train/pos\"\n",
        "sample_files = random.sample(os.listdir(sample_dir), 3)\n",
        "\n",
        "print(\"\\nüîç Preprocessed Reviews:\")\n",
        "for fname in sample_files:\n",
        "    with open(os.path.join(sample_dir, fname), 'r', encoding='utf-8') as f:\n",
        "        raw = f.read()\n",
        "        cleaned = preprocess_text(raw, stopwords)\n",
        "        print(f\"\\nüìù {fname}\\nOriginal:\\n{raw[:200]}\\n\\nCleaned:\\n{cleaned[:200]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUmAHKUema9l",
        "outputId": "a6276ffc-f8b9-46f1-c8d5-b628d2c879f0"
      },
      "id": "HUmAHKUema9l",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Preprocessed Reviews:\n",
            "\n",
            "üìù 5410_7.txt\n",
            "Original:\n",
            "Another Pokemon movie has hit the theaters, and again, I'm hearing the same old, \"Pokemon is dead, blah blah blah.\" The franchise's detractors couldn't be more wrong. Kids are still playing the tradin\n",
            "\n",
            "Cleaned:\n",
            "another pokemon movie hit theaters im hearing old pokemon dead blah blah blah franchises detractors couldnt wrong kids still playing trading card game theyre still watching tv series theyre waiting ga\n",
            "\n",
            "üìù 4681_7.txt\n",
            "Original:\n",
            "Overall, I enjoyed this film and would recommend it to indie film lovers.<br /><br />However, I really want to note the similarities between parts of this film and Nichols' Closer. One scene especiall\n",
            "\n",
            "Cleaned:\n",
            "overall enjoyed film would recommend indie film loversbr br however really want note similarities parts film nichols closer one scene especially adrian greniers character questioning rosario dawsons s\n",
            "\n",
            "üìù 1588_8.txt\n",
            "Original:\n",
            "*What I Like About SPOILERS* Teenager Holly Tyler (Amanda Bynes) goes to live with older sister Valerie (Jennie Garth) to avoid moving to Japan with her father; but she doesn't know the half of the wa\n",
            "\n",
            "Cleaned:\n",
            "like spoilers teenager holly tyler amanda bynes goes live older sister valerie jennie garth avoid moving japan father doesnt know half wacky things happen sister friends gary wesley jonathan tina alis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "impossible-potato",
      "metadata": {
        "id": "impossible-potato"
      },
      "source": [
        "#### Task 1.3: Splitting the dataset (5 points)\n",
        "\n",
        "In this part, divide the given dataset into training and testing sets based on an 80-20 split using python.\n",
        "Print out the sizes of the training dataset and test dataset, training data should contain 40000 reviews and test data should contain 10000 reviews. If your sizes are correct, you get full points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "relevant-episode",
      "metadata": {
        "id": "relevant-episode"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def train_naive_bayes(train_dir, stopwords):\n",
        "    class_word_counts = {\"pos\": defaultdict(int), \"neg\": defaultdict(int)}\n",
        "    class_doc_counts = {\"pos\": 0, \"neg\": 0}\n",
        "    class_total_words = {\"pos\": 0, \"neg\": 0}\n",
        "    vocab = set()\n",
        "\n",
        "    for label in ['pos', 'neg']:\n",
        "        folder = os.path.join(train_dir, label)\n",
        "        files = os.listdir(folder)\n",
        "        class_doc_counts[label] = len(files)\n",
        "\n",
        "        for file in files:\n",
        "            with open(os.path.join(folder, file), 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "                cleaned = preprocess_text(text, stopwords)\n",
        "                words = cleaned.split()\n",
        "\n",
        "                class_total_words[label] += len(words)\n",
        "                for word in words:\n",
        "                    vocab.add(word)\n",
        "                    class_word_counts[label][word] += 1\n",
        "\n",
        "    total_docs = class_doc_counts['pos'] + class_doc_counts['neg']\n",
        "    class_priors = {\n",
        "        'pos': class_doc_counts['pos'] / total_docs,\n",
        "        'neg': class_doc_counts['neg'] / total_docs\n",
        "    }\n",
        "\n",
        "    print(\"Training completed ‚úÖ\")\n",
        "    print(\"Vocabulary size:\", len(vocab))\n",
        "    print(\"Documents - Pos:\", class_doc_counts['pos'], \"| Neg:\", class_doc_counts['neg'])\n",
        "\n",
        "    return class_word_counts, class_total_words, class_priors, vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "julian-venice",
      "metadata": {
        "id": "julian-venice"
      },
      "source": [
        "#### Task 1.4: Create Naive Bayes classifier (30 points)\n",
        "\n",
        "You will create your own Naive Neighbors classifier function by implementing the following algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "southwest-consumption",
      "metadata": {
        "id": "southwest-consumption",
        "outputId": "ddeeeec8-ca6e-48ee-9036-80ceac30e86a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'NBAlgo.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-3924193231.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NBAlgo.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1232\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'NBAlgo.png'"
          ]
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "display(Image(filename='NBAlgo.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "consolidated-fortune",
      "metadata": {
        "id": "consolidated-fortune"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def predict(review, stopwords, class_word_counts, class_total_words, class_priors, vocab):\n",
        "    cleaned = preprocess_text(review, stopwords)\n",
        "    words = cleaned.split()\n",
        "\n",
        "    scores = {}\n",
        "    for label in ['pos', 'neg']:\n",
        "        log_prob = math.log(class_priors[label])\n",
        "        total_words = class_total_words[label]\n",
        "        word_counts = class_word_counts[label]\n",
        "\n",
        "        for word in words:\n",
        "            # Laplace smoothing\n",
        "            word_freq = word_counts.get(word, 0)\n",
        "            prob = (word_freq + 1) / (total_words + len(vocab))\n",
        "            log_prob += math.log(prob)\n",
        "\n",
        "        scores[label] = log_prob\n",
        "\n",
        "    return max(scores, key=scores.get)  # label with higher score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "durable-equivalent",
      "metadata": {
        "id": "durable-equivalent"
      },
      "source": [
        "#### Task 1.5: Implement evaluation functions (10 points)\n",
        "\n",
        "Implement evaluation functions that calculates the:\n",
        "- classification accuracy,\n",
        "- F1 score,\n",
        "- and the confusion matrix\n",
        "of your classifier on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(test_dir, stopwords, class_word_counts, class_total_words, class_priors, vocab):\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for label in ['pos', 'neg']:\n",
        "        folder = os.path.join(test_dir, label)\n",
        "        files = os.listdir(folder)\n",
        "\n",
        "        for file in files:\n",
        "            path = os.path.join(folder, file)\n",
        "            if os.path.isdir(path) or not file.endswith('.txt'):\n",
        "                continue\n",
        "\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "                pred_label = predict(text, stopwords, class_word_counts, class_total_words, class_priors, vocab)\n",
        "\n",
        "                y_true.append(label)\n",
        "                y_pred.append(pred_label)\n",
        "\n",
        "    # Convert to binary (pos=1, neg=0) for sklearn metrics\n",
        "    y_true_bin = [1 if label == 'pos' else 0 for label in y_true]\n",
        "    y_pred_bin = [1 if label == 'pos' else 0 for label in y_pred]\n",
        "\n",
        "    print(\"üìä Evaluation Results:\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_true_bin, y_pred_bin))\n",
        "    print(\"Precision:\", precision_score(y_true_bin, y_pred_bin))\n",
        "    print(\"Recall:\", recall_score(y_true_bin, y_pred_bin))\n",
        "    print(\"F1 Score:\", f1_score(y_true_bin, y_pred_bin))\n",
        "\n"
      ],
      "metadata": {
        "id": "hV7mdSp6nde1"
      },
      "id": "hV7mdSp6nde1",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "angry-diversity",
      "metadata": {
        "id": "angry-diversity"
      },
      "source": [
        "### Part 2:  Naive Bayes classifier using scikit-learn (40 points)\n",
        "\n",
        "In this part, use scikit-learn‚Äôs CountVectorizer to transform your train and test set to bag-of-words representation and Na√Øve Bayes implementation to train and test the Na√Øve Bayes on the provided dataset. Use scikit-learn‚Äôs accuracy_score function to calculate the accuracy and confusion_matrix function to calculate the confusion matrix on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "intelligent-fundamental",
      "metadata": {
        "id": "intelligent-fundamental"
      },
      "outputs": [],
      "source": [
        "# Here are the libraries and specific functions you will be needing for this part\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "tough-crazy",
      "metadata": {
        "id": "tough-crazy"
      },
      "outputs": [],
      "source": [
        "def load_reviews_to_dataframe(base_dir):\n",
        "    reviews = []\n",
        "    labels = []\n",
        "\n",
        "    for label in ['pos', 'neg']:\n",
        "        folder = os.path.join(base_dir, label)\n",
        "        files = os.listdir(folder)\n",
        "\n",
        "        for file in files:\n",
        "            path = os.path.join(folder, file)\n",
        "            if os.path.isdir(path) or not file.endswith(\".txt\"):\n",
        "                continue\n",
        "\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                reviews.append(f.read())\n",
        "                labels.append(1 if label == 'pos' else 0)\n",
        "\n",
        "    return pd.DataFrame({'review': reviews, 'label': labels})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = load_reviews_to_dataframe(\"/content/train\")\n",
        "test_df = load_reviews_to_dataframe(\"/content/test\")\n"
      ],
      "metadata": {
        "id": "6c76IoEwnv_j"
      },
      "id": "6c76IoEwnv_j",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def simple_preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "train_df['review'] = train_df['review'].apply(simple_preprocess)\n",
        "test_df['review'] = test_df['review'].apply(simple_preprocess)\n"
      ],
      "metadata": {
        "id": "hPmdfR4anxeO"
      },
      "id": "hPmdfR4anxeO",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words='english')  # scikit-learn built-in stopwords\n",
        "X_train = vectorizer.fit_transform(train_df['review'])\n",
        "X_test = vectorizer.transform(test_df['review'])\n",
        "y_train = train_df['label']\n",
        "y_test = test_df['label']\n"
      ],
      "metadata": {
        "id": "n9fdCpqzn1Vg"
      },
      "id": "n9fdCpqzn1Vg",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"üìä Scikit-learn Naive Bayes Evaluation:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Negative\", \"Positive\"]))\n"
      ],
      "metadata": {
        "id": "6bnylFGhoHSV",
        "outputId": "bdf0d9d5-75f1-419f-d3c8-9fc7098edd6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6bnylFGhoHSV",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Scikit-learn Naive Bayes Evaluation:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.79      0.88      0.83     12500\n",
            "    Positive       0.86      0.77      0.81     12500\n",
            "\n",
            "    accuracy                           0.82     25000\n",
            "   macro avg       0.83      0.82      0.82     25000\n",
            "weighted avg       0.83      0.82      0.82     25000\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}