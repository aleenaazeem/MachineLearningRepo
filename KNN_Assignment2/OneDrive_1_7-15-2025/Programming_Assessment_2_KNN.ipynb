{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYc97WFJYC-f"
      },
      "source": [
        "# Machine Learning\n",
        "## Programming Assignment 2: KNN\n",
        "\n",
        "#### Instructions:\n",
        "The aim of this assignment is to give you hands-on experience with a real-life machine learning application.\n",
        "You will be analyzing the sentiment of tweets using KNN classification.\n",
        "You can only use the Python programming language and Jupyter Notebooks.\n",
        "Please use procedural programming style and comment your code thoroughly.\n",
        "There are two parts of this assignment. In part 1, you can use NumPy, Pandas, Matplotlib, and any other standard Python libraries. You are not allowed to use NLTK, scikit-learn, or any other machine learning toolkit. You can only use scikit-learn in part 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71aWGqftYC-l"
      },
      "source": [
        "### Part 1: Implementing KNN classifier from scratch (75 marks)\n",
        "\n",
        "You are not allowed to use scikit-learn or any other machine learning toolkit for this part. You have to implement your own KNN classifier from scratch. You may use Pandas, NumPy, Matplotlib, and other standard Python libraries.\n",
        "\n",
        "#### Problem:\n",
        "The purpose of this assignment is to get you familiar with the k nearest neighbor classification. You are given the ‘Apple Sentiment Tweets’ dataset that contains around 1630 tweets about Apple labeled as positive, neutral and negative in the form of 1, 0, and -1 respectively. Your task is to implement the k nearest classifier and use it for predicting the sentiments of the tweets about Apple.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N3i9vY0YC-n"
      },
      "outputs": [],
      "source": [
        "## Here are the libraries you will need for this part/\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.spatial as sc\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import random\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Q0l7n_YC-p"
      },
      "source": [
        "#### Task 1.1: Dataset (5 points)\n",
        "The dataset contains around 1,630 tweets. There are only two columns in the dataset:\n",
        "Text: Contains the text of the tweet\n",
        "Sentiment: Contains the sentiment of the tweet which is divided into three classes: 1 (positive), -1 (negative), and 0 (neutral).\n",
        "\n",
        "Your task is to read the dataset and stopwords file into a useful data structure. Print out a few tweets and a few items from the stop word list, succesfully being able to do this will earn you 5 points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2HquGKyYC-q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EljMqK2wYC-r"
      },
      "source": [
        "#### Task 1.2: Data Preprocessing (10 points)\n",
        "\n",
        "In the preprocessing step, you’re required to remove the stop words, punctuation marks, numbers, unwanted symbols, hyperlinks, and usernames from the tweets and convert them to lower case. You may find the string and regex module useful for this purpose. Use the stop word list provided within the assignment.\n",
        "\n",
        "Print out a few random tweets from your dataset, if they conform to the rules mentioned above, you will gain 10 points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEME-wqzYC-r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTEWPNSeYC-s"
      },
      "source": [
        "#### Task 1.3: Splitting the dataset (5 points)\n",
        "\n",
        "In this part, divide the given dataset into training and testing sets based on an 80-20 split using python.\n",
        "Print out the sizes of the training dataset and test dataset, training data should contain 1304 tweets and test data should contain 326 tweets. If your sizes are correct, you get full points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qhg1EkwYC-t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoINrWPEYC-u"
      },
      "source": [
        "#### Task 1.4: Feature Extraction (10 points)\n",
        "\n",
        "In the feature extraction step, you’ll represent each tweet as a bag-of-words (BoW), that is, an unordered set of words with their position ignored, keeping only their frequency in the tweet. For example, consider the below tweets:\n",
        "T1 = Welcome to machine learning!\n",
        "T2 = kNN is a powerful machine learning algorithm.\n",
        "The bag-of-words representation (ignoring numbers, case, and punctuation) for the above tweets are:\n",
        "\n",
        "| Vocabulary | welcome | To | machine | learning | knn | is | a | powerful | algorithm |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| T1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 |\n",
        "| T2 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n",
        "\n",
        "\n",
        "Note: We only use the training set to construct the vocabulary for the BoW representation.\n",
        "\n",
        "Print out the vocabulary as well as the bow representation for a random tweet. Getting the correct output will result in full credit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Epq0oRzPYC-v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM4NeAN4YC-v"
      },
      "source": [
        "#### Task 1.5: Create KNN classifier (10 points)\n",
        "\n",
        "You will create your own k-Nearest Neighbors classifier function by performing the following tasks:\n",
        "- For a test data point, find its distance from all training instances.\n",
        "- Sort the calculated distances in ascending order based on distance values.\n",
        "- Choose k training samples with minimum distances from the test data point.\n",
        "- Return the most frequent class of these samples. (Your function should work with Euclidean distance as well as Manhattan distance. Pass the distance metric as a parameter in the KNN classifier function. Your function should also be general enough to work with any value of k.)\n",
        "- For the even values of k given in the above task, break ties by backing off to the k-1 value. (For example, if you have k = 6 nearest neighbors and three of them have the label ‘positive’ and three have the label ‘negative, then you will break off the tie by taking k=5 nearest neighbors. On the other hand, let's say if you have k = 6 nearest neighbors where two have the label ‘positive’, two have the label ‘negative’, and two have the label ‘neutral’. In that case, k =5 will still lead to two labels having a draw in which case you will continue decreasing k until there is a clear winner.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WIoQVttYC-v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrVI15bkYC-w"
      },
      "source": [
        "#### Task 1.6: Implement evaluation functions (10 points)\n",
        "\n",
        "Implement evaluation functions that calculates the:\n",
        "- classification accuracy,\n",
        "- F1 score,\n",
        "- and the confusion matrix\n",
        "of your classifier on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpMA-K8SYC-w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc-3z9FxYC-x"
      },
      "source": [
        "#### Task 1.7: Cross Validation (15 points)\n",
        "\n",
        "Use 5- fold cross-validation on your training data. (In cross-validation, you divide the training data set into 5 parts. 4 parts will be used for training and 1 part will be used for validation. Then you will take a different part of your data as a validation data set and train your algorithm on the rest of the data set.) Run your KNN function for this data for the values of k = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. Do this for both the Euclidean distance and the Manhattan distance for each value of k.\n",
        "\n",
        "Run your evaluation function for each value of k for both distance metrics, Report classification accuracy, F1 score, and confusion matrix.\n",
        "\n",
        "Present the results as a graph with k values on the x-axis and classification accuracy on the y-axis. Use a single plot to compare the two versions of the classifier (one using Euclidean and the other using Manhattan distance metric). Make another graph but with the F1 score on the y-axis this time. The graphs should be properly labelled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWbaPpkBYC-y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5WGB7-wYC-y"
      },
      "source": [
        "#### Task 1.8: Classification (10 points)\n",
        "\n",
        "Finally, use the best value of k for both distance metrics and run it on the test data set. Find the F1 score, classification accuracy, and confusion matrix and print them.\n",
        "\n",
        "You accuracy should be above 75 and f1 score should be above 60 to get full points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_yqG4LIYC-y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiMI4XddYC-z"
      },
      "source": [
        "### Part 2:  kNN classifier using scikit-learn (25 points)\n",
        "\n",
        "In this part, you have to use scikit-learn’s KNN implementation to train and test your classifier on the dataset used in Part 1. Repeat the tasks you have done in Part 1 but this time using scikit-learn. Use your bag of words to do cross-validation and run the KNN classifier for values of k = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 using both Euclidean and Manhattan distance. Use scikit-learn’s accuracy score function to calculate the accuracy, classification_report to calculate macro-average (precision, recall, and F1), and confusion matrix function to calculate confusion matrix for test data. Also present the results as a graph with k values on the x-axis and performance measures on the y-axis just like you did in part 1. Use a single plot to compare the two versions of the classifier (one using Euclidean and the other using Manhattan distance metric). Finally, print the best values of k for both distance metrics. Then use this value of k on the test data set and print the evaluation scores.\n",
        "\n",
        "To get full marks, the accuracy score, classification reports and confusion matrix must be shown for both distance metrics and values for accuracy and F1 should be similar to those obtained in the previous part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8s9tCKTYC-z"
      },
      "outputs": [],
      "source": [
        "# Here are the libraries and specific functions you will be needing for this part\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0PQAkSdYC-z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jiMI4XddYC-z"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}