How to check classifiers‚Ä¶ the measures taken.
1. Matthews Correlation Coefficient (MCC)
‚Ä¢	Measures the quality of binary classifications
‚Ä¢	Takes into account all four confusion matrix categories (TP, TN, FP, FN)
‚Ä¢	Range: -1 (worst) to +1 (perfect); 0 = random
‚Ä¢	Good for imbalanced datasets
2. Cohen‚Äôs Kappa
‚Ä¢	Measures agreement between predicted and actual labels, adjusted for chance agreement
‚Ä¢	Especially useful when evaluating human vs model agreement
3. Balanced Accuracy
‚Ä¢	Average of sensitivity (recall) and specificity
‚Ä¢	Good when classes are imbalanced
4. Log Loss (Cross-Entropy Loss)
‚Ä¢	Used when model predicts probabilities (not just classes)
‚Ä¢	Penalizes confident but wrong predictions heavily
‚Ä¢	Lower = better
5. Brier Score
‚Ä¢	Measures probability accuracy (how close your predicted probabilities are to true labels)
‚Ä¢	Lower = better
‚Ä¢	Often used in calibration of classifiers
6. G-Mean (Geometric Mean)
‚Ä¢	Geometric mean of sensitivity and specificity
‚Ä¢	Used for imbalanced datasets to ensure both classes are treated fairly
7. Top-K Accuracy (Top-1, Top-5, etc.)
‚Ä¢	Common in image classification (e.g., ImageNet)
‚Ä¢	‚ÄúWas the correct label in the top K predictions?‚Äù
8. Average Precision (AP) / Mean Average Precision (mAP)
‚Ä¢	Used in object detection, ranking, and retrieval
‚Ä¢	Average of precisions at different thresholds
9. Precision@K, Recall@K
‚Ä¢	Popular in recommender systems or search engines
‚Ä¢	How many of the top-K recommendations were actually relevant?
10. Hamming Loss
‚Ä¢	Used in multi-label classification
‚Ä¢	Fraction of wrong labels to total labels

Use Case	Recommended Metrics
Imbalanced datasets	MCC, F1, Balanced Accuracy, G-Mean
Probabilistic predictions	Log Loss, Brier Score, ROC-AUC
Multi-class classification	Macro/Micro F1, Top-K Accuracy, Cohen‚Äôs Kappa
Ranking or recommendations	Precision@K, mAP, Recall@K
Multi-label classification	Hamming Loss, Subset Accuracy, Jaccard Score

Metric	Formula	Think of it as...	Easy Mnemonic üí°
Accuracy	(TP + TN) / (TP + FP + FN + TN)	How often your prediction is overall correct	‚ÄúHow many total correct‚Äù
Precision	TP / (TP + FP)	Of all predicted positives, how many were actually correct	‚ÄúTrust your positives‚Äù
Recall (Sensitivity)	TP / (TP + FN)	Of all real positives, how many did you catch?	‚ÄúCatch all real cases‚Äù
Specificity	TN / (TN + FP)	Of all real negatives, how many did you correctly ignore?	‚ÄúIgnore negatives well‚Äù
F1 Score	2 √ó (Precision √ó Recall) / (Precision + Recall)	Balances precision and recall (harmonic mean)	‚ÄúTrade-off score‚Äù
ROC Curve	Plot of TPR vs. FPR	Visual curve showing model performance at all thresholds	‚ÄúCurve for classifier quality‚Äù
AUC	Area under ROC	One number summarizing the ROC curve ‚Äî 1 is best	‚ÄúBigger area = better‚Äù

Feature	ROC Curve	Precision-Recall Curve
X-axis	False Positive Rate (FPR) = FP / (FP + TN)	Recall = TP / (TP + FN)
Y-axis	True Positive Rate (TPR) = Recall	Precision = TP / (TP + FP)
Best Use Case	When classes are balanced	When classes are imbalanced
Focuses On	Trade-off between TPR and FPR	Trade-off between Precision and Recall
Baseline	Diagonal line (AUC = 0.5 = random guess)	Horizontal line (baseline = class prevalence)
Interpretation	How well can the model distinguish classes?	How good are positive predictions?

