{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYHCKyuYE-lF"
      },
      "source": [
        "# Machine Learning\n",
        "## Programming Assignment 4: Logistic Regression\n",
        "\n",
        "### Instructions:\n",
        "The aim of this assignment is to give you hands-on experience with a real-life machine learning application. You will be using Logistic Regression classifier to predict digits based on their drawing.\n",
        "You can only use the Python programming language and Jupyter Notebooks. Please use procedural programming style and comment your code thoroughly. There are two parts of this assignment. In part 1, you can use NumPy, Pandas, Matplotlib, and any other standard Python libraries. You are not allowed to use NLTK, scikit-learn, or any other machine learning toolkit. You can only use scikit-learn in part 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTXoy6nUE-lH"
      },
      "source": [
        "### Part 1: Implementing Logistic Regression classifier from scratch (75 marks)\n",
        "\n",
        "You are not allowed to use scikit-learn or any other machine learning toolkit for this part. You have to implement your own Logistic Regression classifier from scratch. You may use Pandas, NumPy, Matplotlib, and other standard Python libraries.\n",
        "\n",
        "#### Problem:\n",
        "The purpose of this assignment is to get you familiar with the Logistic Regression classification. You are given the ‘MNIST’ dataset that contains around 60000 hand drawn digits. Your task is to implement the Logistic Regression classifier and use it for predicting the digits based on their drawing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCwEtU4jE-lI"
      },
      "outputs": [],
      "source": [
        "## Here are the libraries you will need for this part/\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.spatial as sc\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import random\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WTh1_gsE-lK"
      },
      "source": [
        "#### Task 1.1: Dataset (5 points)\n",
        "The MNIST digits dataset is a widely used benchmark dataset in the field of machine learning and computer vision. MNIST stands for Modified National Institute of Standards and Technology, which is the organization that collected and curated the dataset. The MNIST dataset consists of a collection of 60,000 handwritten digit images for training and an additional 10,000 images for testing. These images are grayscale and have a fixed size of 28x28 pixels. Each image represents a single handwritten digit ranging from 0 to 9.  Each image in the dataset is accompanied by its corresponding label, indicating the digit it represents. The labels are represented as integers from 0 to 9, matching the handwritten digit in the image. This labeling allows for supervised learning tasks, where algorithms can learn to classify and recognize handwritten digits based on the provided training examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pr_Y4C6E-lK"
      },
      "source": [
        "#### Task 1.2: Data Preprocessing (10 points)\n",
        "\n",
        "In the preprocessing step, you’re required to load the data, and scale the values using the StandardScalar function present in scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8MD2LNKE-lL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl3QsT7pE-lL"
      },
      "source": [
        "#### Task 1.3: Splitting the dataset (5 points)\n",
        "\n",
        "In this part, divide the given dataset into training and testing sets using python.\n",
        "Print out the sizes of the training dataset and test dataset, training data should contain 50000 images and test data should contain 10000 images. If your sizes are correct, you get full points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-VxOnTaE-lL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-yeVI-DE-lL"
      },
      "source": [
        "#### Task 1.4: Create Logistic Regression classifier (20 points)\n",
        "\n",
        "Implement Multinomial Logistic Regression from scratch keeping in view all the discussions\n",
        "from the lectures to classify the images into the 10 classes specified. Specifically, you’ll need to implement the\n",
        "following: <br>\n",
        "● Softmax function <br>\n",
        "● Cross-entropy loss function (for multinomial logistic regression) <br>\n",
        "● Batch Gradient Descent <br>\n",
        "● Prediction function that predicts the label of test recordings using learned\n",
        "multinomial logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ht7IGzZwE-lM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIFAMfDeE-lM"
      },
      "source": [
        "#### Task 1.5: Implement evaluation functions (10 points)\n",
        "\n",
        "Implement evaluation functions that calculates the:\n",
        "- classification accuracy,\n",
        "- F1 score,\n",
        "- and the confusion matrix\n",
        "of your classifier on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TpPqZajE-lM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ-DazinE-lM"
      },
      "source": [
        "#### Task 1.6: Cross Validation (15 points)\n",
        "\n",
        "Use 5- fold cross-validation on your training data. (In cross-validation, you divide the training data set into 5 parts. 4 parts will be used for training and 1 part will be used for validation. Then you will take a different part of your data as a validation data set and train your algorithm on the rest of the data set.) Run your Logistic Regression function for this data for 5 learning rate values ranging from 0.001 and 0.01.\n",
        "\n",
        "Run your evaluation function for each value of the learning rate, Report classification accuracy, F1 score, and confusion matrix.\n",
        "\n",
        "Present the results as a graph with learning rate values on the x-axis and classification accuracy on the y-axis. Make another graph but with the F1 score on the y-axis this time. The graphs should be properly labelled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YztrJXYME-lN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0Ti9An0E-lN"
      },
      "source": [
        "#### Task 1.7: Classification (10 points)\n",
        "\n",
        "Finally, use the best value of the learning rate and run it on the test data set. Find the F1 score, classification accuracy, and confusion matrix and print them.\n",
        "\n",
        "You accuracy should be above 80 and f1 score should be above 70 to get full points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXia4d06E-lN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEMIWNYlE-lN"
      },
      "source": [
        "### Part 2:  Logistic Regression classifier using scikit-learn (25 points)\n",
        "\n",
        "Use scikit-learn’s Logistic Regression implementation to train and test the logistic regression\n",
        "on the provided dataset. Use scikit-learn’s accuracy_score function to calculate the accuracy\n",
        "and confusion_matrix function to calculate confusion matrix on the test set.\n",
        "To get full marks, the accuracy score, classification reports and confusion matrix must be shown for values for accuracy and F1 should be similar to those obtained in the previous part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB31orpOE-lN"
      },
      "outputs": [],
      "source": [
        "# Here are the libraries and specific functions you will be needing for this part\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiMEia_wE-lO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}